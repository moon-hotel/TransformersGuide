# å¿«é€ŸæŒ‡å¼•

è®©æˆ‘ä»¬å¿«é€Ÿçš„æ¥çœ‹ä¸€çœ‹*Transformers*åº“çš„ç‰¹æ€§ã€‚é€šè¿‡*Transformers*åº“ï¼Œä½ å¯ä»¥ä¸‹è½½ä¸€äº›é¢„è®­ç»ƒæ¨¡å‹æ¥å®Œæˆè‡ªç„¶è¯­è¨€ç†è§£ä¸­çš„å¸¸è§ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼šæ–‡æœ¬è¯­ä¹‰åˆ†æã€è‡ªç„¶è¯­è¨€ç”Ÿæˆã€è¯­ä¹‰æ¨æ–­å’Œæ–‡æœ¬ç¿»è¯‘ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°å¦‚ä½•è½»æ¾çš„é€šè¿‡ç®¡é“ï¼ˆpipeline APIï¼‰æ¥å¿«é€Ÿçš„ä½¿ç”¨è¿™äº›é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨æ–­é¢„æµ‹ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä¼šæ·±å…¥çš„æ¢ç©¶*Transformers*æ˜¯å¦‚ä½•æ¥åˆ©ç”¨è¿™äº›æ¨¡å‹æ¥å¤„ç†ä½ çš„æ•°æ®çš„ã€‚

**æ³¨æ„ï¼š**ç¤ºä¾‹ä¸­åˆ—å‡ºçš„æ‰€æœ‰ä»£ç å‡å¯ä»¥é€šè¿‡å·¦ä¸Šè§’çš„æŒ‰é’®åœ¨Pytorchå’ŒTensorFlowä¹‹é—´åˆ‡æ¢ã€‚å¦‚æœæ²¡æœ‰æŒ‰é’®ï¼Œè¿™è¡¨ç¤ºè¯¥ä»£ç å¯ä»¥åœ¨ä¸¤ä¸ªå¹³å°ä¸Šé€šç”¨ã€‚(è¯‘è€…æ³¨ï¼šåœ¨è¯‘æ–‡ä¸­ä¸¤ç§ä»£ç è´´åœ¨äº†åŒä¸€å¤„ï¼Œå¹¶è¿›è¡Œäº†æ³¨é‡Š)

## <span id = '011'>1 é€šè¿‡ç®¡é“æ¥å¼€å§‹æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªä»»åŠ¡</span>

ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯é€šè¿‡`pipeline()`æ¥è¿›è¡Œå®ç°ã€‚

é€šè¿‡*Transformers*ä½ å¯ä»¥å®Œæˆå¦‚ä¸‹æ‰€ç¤ºçš„ä»»åŠ¡å»ºæ¨¡ï¼š

- è¯­ä¹‰åˆ†æï¼šä¸€æ®µæ–‡æœ¬ä¸ºæ­£é¢å€¾å‘è¿˜æ˜¯è´Ÿé¢å€¾å‘ï¼Ÿ
- æ–‡æœ¬ç”Ÿæˆï¼ˆè‹±è¯­ï¼‰ï¼šç»™å‡ºéƒ¨åˆ†æç¤ºï¼Œæ¨¡å‹å°†è‡ªåŠ¨ä¸ºå…¶ç”Ÿæˆåç»­æ–‡æœ¬ã€‚
- å‘½åä½“è¯†åˆ«ï¼šå¯¹äºè¾“å…¥çš„æ–‡æœ¬ï¼Œæ¨¡å‹ä¼šæ ‡è®°å¯¹åº”çš„æ–‡æœ¬åºåˆ—æ‰€è¡¨ç¤ºçš„å®ä½“ï¼ˆäººç‰©ã€åœ°ç‚¹ç­‰ï¼‰ã€‚
- é—®é¢˜å›ç­”ï¼šå°†æ–‡ç« å’Œå¯¹åº”é—®é¢˜è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œæ¨¡å‹è¾“å‡ºé—®é¢˜æ‰€å¯¹åº”çš„å›ç­”ã€‚
- ç©ºç™½æ¨æ–­ï¼šè¾“å…¥ä¸€æ®µæŸäº›è¯è¯­è¢«é®è”½ï¼ˆå¦‚ç”¨`[MASK]`è¿›è¡Œæ›¿ä»£ï¼‰çš„æ–‡æœ¬ï¼Œæ¨¡å‹æ¨æ–­å¤„é®è”½éƒ¨åˆ†çš„å†…å®¹
- æ‘˜è¦ç”Ÿæˆï¼šå¯¹è¾“å…¥çš„é•¿æ–‡æœ¬è¾“å‡ºå…¶å¯¹åº”çš„æ‘˜è¦å†…å®¹ã€‚
- åºåˆ—ç¿»è¯‘ï¼šå°†ä¸€æ®µæ–‡æœ¬åºåˆ—ç¿»è¯‘æˆå¦å¤–ä¸€ç§è¯­è¨€ã€‚
- ç‰¹å¾æŠ½å–ï¼šè¾“å…¥ä¸€æ®µæ–‡æœ¬ï¼Œè¿”å›ä¸€ä¸ªèƒ½è¡¨ç¤ºè¯¥æ–‡æœ¬çš„ç‰¹å¾å‘é‡ã€‚

ä¸‹é¢ï¼Œç„¶æˆ‘ä»¬ä¸€èµ·æ¥çœ‹çœ‹å¦‚ä½•é€šè¿‡*Transformers*æ¥å®Œæˆè¯­ä¹‰åˆ†æè¿™ä¸€ä»»åŠ¡ï¼ˆå…¶å®ƒä»»åŠ¡ä»‹ç»å¯æŸ¥çœ‹[ä»»åŠ¡æ€»ç»“-----------]()ï¼‰ã€‚

```python
from transformers import pipeline
classifier = pipeline('sentiment-analysis')
```

å½“ä½ ç¬¬ä¸€æ¬¡é”®å…¥ä¸Šè¿°ä»£ç å¹¶è¿è¡Œæ—¶ï¼Œå…¶å¯¹åº”çš„é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆtokenizerï¼‰æ‰€éœ€è¦çš„è¯è¡¨éƒ½ä¼šè¢«ä¸‹è½½å¹¶ç¼“å­˜åˆ°æœ¬åœ°ï¼Œå¯¹äºè¿™éƒ¨åˆ†æˆ‘ä»¬ç¨åå†è¿›è¡Œä»‹ç»ã€‚ä½†æ˜¯é¦–å…ˆéœ€è¦æ˜ç™½çš„å°±æ˜¯åˆ†è¯å™¨çš„ä½œç”¨æ˜¯å°†è¾“å…¥çš„æ–‡æœ¬åºåˆ—è¿›è¡Œé¢„å¤„ç†ï¼ˆè¯‘è€…æ³¨ï¼šä¸­æ–‡çš„è¯æ˜¯åˆ‡åˆ†æˆä»¥å­—ä¸ºå•ä½ï¼Œè‹±æ–‡çš„è¯å°±æ˜¯å•è¯ï¼‰ï¼Œç„¶åå†å°†å…¶è¾“å…¥åˆ°æ¨¡å‹ä¸­ç”¨äºé¢„æµ‹ã€‚åœ¨*Transformers*ä¸­ï¼Œå¯ä»¥é€šè¿‡ç®¡é“æ¥å°†æ‰€æœ‰çš„å¤„ç†æ­¥éª¤ç»“åˆåˆ°ä¸€èµ·ï¼ŒåŒæ—¶è¿˜èƒ½ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿç›´è§‚å¾—çœ‹åˆ°é¢„æµ‹åçš„ç»“æœã€‚

```python
classifier('We are very happy to show you the ğŸ¤— Transformers library.')

[{'label': 'POSITIVE', 'score': 0.9997795224189758}]
```

ä»¤äººæŒ¯å¥‹çš„æ˜¯ï¼Œè¿˜å¯ä»¥ç›´æ¥é€šè¿‡`list`æ¥è¾“å…¥å¤šä¸ªæ ·æœ¬ã€‚è¿™äº›æ ·æœ¬åœ¨ç»è¿‡é¢„å¤„ç†åï¼Œå°†ä¼šä½œä¸ºä¸€ä¸ªbatchçš„æ•°æ®è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œç„¶åè¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç»“æœçš„å­—å…¸ã€‚

```python
results = classifier(["We are very happy to show you the ğŸ¤— Transformers library.","We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")
    
label: POSITIVE, with score: 0.9998
label: NEGATIVE, with score: 0.5309
```

ä½ å¯ä»¥çœ‹åˆ°ä¸Šé¢çš„ç¬¬äºŒä¸ªå¥å­å·²ç»è¢«åˆ†ç±»æˆä¸ºNegativeçš„æ ‡ç­¾ï¼ˆæ³¨æ„ï¼Œæ¨¡å‹åªä¼šå°†å…¶åˆ†ç±»æˆæ­£é¢æˆ–è€…æ˜¯è´Ÿé¢ï¼‰ï¼Œä½†æ˜¯å®ƒçš„å¾—åˆ†å´éå¸¸çš„åä¸­æ€§ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨ä¸Šè¿°è¿‡ç¨‹ä¸­`pipeline()`ä¸‹è½½çš„éƒ½æ˜¯ä¸€ä¸ªå«åš`distilbert-base-uncased-finetuned-sst-2-english`çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¿™ä¸ª[é¡µé¢](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)ä¸­æŸ¥çœ‹åˆ°æ›´å¤šå…³äºè¯¥ä¸è®­ç»ƒæ¨¡å‹çš„ç›¸å…³ä¿¡æ¯ã€‚è¿™ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ˜¯é€šè¿‡ [DistilBERT architecture](https://huggingface.co/transformers/model_doc/distilbert.html) ç½‘ç»œæ‰€è®­ç»ƒå¾—åˆ°çš„ï¼Œå¹¶ä¸”å·²ç»åœ¨æ•°æ®é›†SST-2ä¸Šè¿›è¡Œäº†å¾®è°ƒä»¥ä¾¿äºæ›´å¥½çš„è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ã€‚

ç°åœ¨å‡è®¾æˆ‘ä»¬æƒ³è¦ä½¿ç”¨å¦å¤–æ¨¡å‹ï¼ˆä¾‹å¦‚ä¸€ä¸ªå·²ç»åœ¨æ³•æ–‡è¯­æ–™ä¸Šè®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬åªéœ€è¦å¯åœ¨é¡µé¢ä¸­[model hub](https://huggingface.co/models) ä»¥å…³é”®è¯"French"å’Œâ€œtext-classifictionâ€è¿›è¡Œæœç´¢ï¼Œå®ƒå°±ä¼šè¿”å›ç›¸åº”çš„æ¨¡å‹å»ºè®®ã€‚ä¾‹å¦‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­å°±ä¼šå¾—åˆ°`nlptown/bert-base-multilingual-uncased-sentiment`å»ºè®®ã€‚ä¸‹é¢ç„¶æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•æ¥ä½¿ç”¨è¿™ä¸€æ¨¡å‹å§ã€‚

ä½ å¯ä»¥é€šè¿‡`pipeline()`ç›´æ¥å°†è¿™ä¸€æ¨¡å‹çš„åå­—ä½œä¸ºå‚æ•°ä¼ å…¥ï¼š

```python
classifier = pipeline('sentiment-analysis', model="nlptown/bert-base-multilingual-uncased-sentiment")
```

ç°åœ¨ï¼Œä¸Šé¢å®šä¹‰å¥½çš„è¿™ä¸ªåˆ†ç±»å™¨å°±èƒ½å¤Ÿå®Œæˆå¯¹äºè‹±è¯­ã€æ³•è¯­ã€è·å…°è¯­ã€å¾·è¯­ã€æ„å¤§åˆ©è¯­å’Œè¥¿ç­ç‰™è¯­æ–‡æœ¬çš„æƒ…æ„Ÿåˆ†ç±»å·¥ä½œã€‚å½“ç„¶ï¼Œè¿˜å¯ä»¥å°†å‚æ•°`model=`æ›¿æ¢ä¸ºè‡ªå·±ä¿å­˜åœ¨æœ¬åœ°çš„å·²ç»é¢„è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆè§åæ–‡ï¼‰æ¥å®Œæˆä¸Šè¿°å·¥ä½œã€‚åŒæ—¶ï¼Œä½ è¿˜å¯ä»¥ä¼ å…¥ä¸€ä¸ªå®é™…çš„æ¨¡å‹å¯¹è±¡å’Œå…¶å¯¹åº”åˆ†è¯å™¨ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ

å¯¹äºåé¢è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬éœ€è¦ä¼ å…¥ä¸¤ä¸ªç±»å¯¹è±¡åˆ°`pipeline()`ä¸­ã€‚ç¬¬ä¸€ä¸ªæ˜¯ç±»[`AutoTokenizer`](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoTokenizer)ï¼Œæˆ‘ä»¬å°†ç”¨å®ƒæ¥ä¸‹è½½æˆ‘ä»¬æŒ‡å®šæ¨¡å‹æ‰€å¯¹åº”çš„åˆ†è¯å™¨ï¼Œå¹¶å®ä¾‹åŒ–æˆ‘ä»¬æŒ‡å®šçš„æ¨¡å‹ã€‚ç¬¬äºŒä¸ªæ˜¯ç±» [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoModelForSequenceClassification)ï¼ˆæˆ–è€…æ˜¯[`TFAutoModelForSequenceClassification`](https://huggingface.co/transformers/model_doc/auto.html#transformers.TFAutoModelForSequenceClassification)ï¼Œå¦‚æœä½ æ­£åœ¨ä½¿ç”¨TensorFlowï¼‰ï¼Œæˆ‘ä»¬å°†é€šè¿‡è¿™ä¸ªç±»æ¥å®Œæˆå…¶å¯¹åº”æ¨¡å‹çš„ä¸‹è½½ã€‚

**æ³¨æ„ï¼š**å¦‚æœæˆ‘ä»¬éœ€è¦ä½¿ç”¨*Transformers*æ¥å®Œæˆå…¶å®ƒç›¸å…³ä»»åŠ¡ï¼Œé‚£ä¸Šè¿°é…ç½®å°†ä¼šå‘ç”Ÿå˜åŒ–ã€‚åœ¨[ä»»åŠ¡æ€»ç»“](https://huggingface.co/transformers/task_summary.html)é¡µé¢ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å“ªç§ä»»åŠ¡éœ€è¦é‚£ç§é…ç½®ã€‚

```python
# é’ˆå¯¹Pytorchçš„ä»£ç 
from transformers import AutoTokenizer, AutoModelForSequenceClassification

#é’ˆå¯¹TensorFlowçš„ä»£ç 
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡[`from_pretrained()`](https://huggingface.co/transformers/model_doc/auto.html#transformers.AutoModelForSequenceClassification.from_pretrained)æ–¹æ³•æ¥ä¸‹è½½å‰é¢æˆ‘ä»¬æœç´¢åˆ°çš„æ¨¡å‹ï¼Œä»¥åŠå…¶å¯¹åº”çš„åˆ†è¯å™¨ã€‚åŒæ—¶ï¼Œä½ è¿˜å¯ä»¥å°†`model_name`æ›¿æ¢ä¸ºå…¶å®ƒä»»ä½•ä½ å¯ä»¥åœ¨model hubæ‰€èƒ½æœç´¢åˆ°çš„æ¨¡å‹ï¼Œå¯¹æ­¤ä½ å¯ä»¥æ”¾å¿ƒçš„è¿›è¡Œå°è¯•ã€‚

```python
#é’ˆå¯¹Pytorchçš„ä»£ç 
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)

#é’ˆå¯¹TensorFlowçš„ä»£ç 
model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
# This model only exists in PyTorch, so we use the `from_pt` flag to import that model in TensorFlow.
model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)
```

æ¥ç€ï¼Œä½ å°±å¯ä»¥åƒä¸Šé¢çš„ç¤ºä¾‹ä»£ç ä¸€æ ·ï¼Œå¯¹ä½ çš„ç›®æ ‡æ–‡æœ¬è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ã€‚

å¦‚æœä½ åœ¨model hubé‡Œä¸èƒ½å¤Ÿæ‰¾åˆ°ä¸ä½ çš„æ•°æ®ç±»ä¼¼çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆè¯‘è€…æ³¨ï¼šè¿™é‡Œåº”è¯¥æŒ‡çš„æ˜¯æ²¡æœ‰åœ¨ç‰¹å®šä»»åŠ¡ä¸­ç»è¿‡å¾®è°ƒåçš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯æŒ‡åŸå§‹è¯­æ–™ä¸‹è®­ç»ƒå¾—åˆ°çš„é€šç”¨æ¨¡å‹ï¼‰ï¼Œé‚£ä¹ˆä½ å°±éœ€è¦åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒè®­ç»ƒã€‚å¯¹æ­¤ï¼Œæˆ‘ä»¬ä¸“é—¨æä¾›äº†ä¸€äº›[ç¤ºä¾‹è„šæœ¬](https://huggingface.co/transformers/examples.html)æ¥å®Œæˆè¿™äº›ä»»åŠ¡ã€‚ä¸€æ—¦ä½ åœ¨è‡ªå·±çš„æ•°æ®ä¸Šå®Œæˆå¾®è°ƒä¹‹åï¼Œåƒä¸‡ä¸è¦å¿˜äº†å°†å…¶åˆ†äº«åˆ°model hubç¤¾åŒºã€‚è¯¦ç»†åˆ†äº«ä¸Šæ¬¡æ­¥éª¤å¯å‚åŠ [æ­¤å¤„](https://huggingface.co/transformers/model_sharing.html)ã€‚

## <span id = '012'>2 é¢„è®­ç»ƒæ¨¡å‹çš„å†…å¹•</span>

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹åœ¨ä½¿ç”¨`pipeline()`çš„è¿‡ç¨‹ä¸­ï¼Œå…¶èƒŒååˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆäº‹æƒ…ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šé¢æ‰€è¯´åˆ°çš„ï¼Œæ¨¡å‹å’Œåˆ†è¯å™¨éƒ½æ˜¯é€šè¿‡å¯¹åº”çš„`from_pretrained()`æ–¹æ³•æ‰€å»ºç«‹çš„ï¼š

```python
# é’ˆå¯¹Pytorch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

#é’ˆå¯¹TensorFlow
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

```

### 2.1 ä½¿ç”¨åˆ†è¯å™¨

åœ¨å‰é¢æˆ‘ä»¬æåˆ°ï¼Œåˆ†è¯å™¨çš„ä½œç”¨å°±æ˜¯ç”¨æ¥å¯¹è¾“å…¥çš„åŸå§‹æ–‡æœ¬è¿›è¡Œå¤„ç†ï¼Œå…¶å…·ä½“æ­¥éª¤ä¸ºï¼š

é¦–å…ˆï¼Œåˆ†è¯å™¨ä¼šå°†è¾“å…¥çš„æ–‡æœ¬åˆ†å‰²æˆä¸€ä¸ªä¸€ä¸ªçš„è¯ï¼ˆæˆ–è€…æ˜¯æ ‡ç‚¹ï¼‰ï¼Œé€šå¸¸æˆ‘ä»¬æŠŠè¿™ä¸€è¿‡ç¨‹å«åš*tokens*ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿæä¾›äº†å¾ˆå¤šä¸åŒçš„æ–‡æœ¬åˆ†å‰²è§„åˆ™æ¥å¤„ç†ä½ è‡ªå·±çš„æ•°æ®ï¼Œä½ å¯ä»¥åœ¨é¡µé¢[tokenizer summary](https://huggingface.co/transformers/master/tokenizer_summary.html)ä¸­æ‰¾åˆ°æ›´å¤šè¿™æ–¹é¢çš„ä»‹ç»ã€‚å› æ­¤è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬åœ¨è½½å…¥åˆ†è¯å™¨ï¼ˆtokenizerï¼‰çš„æ—¶å€™éœ€è¦æŒ‡å®šä¸€ä¸ªæ¨¡å‹åï¼ˆ`model_name`ï¼‰æ¥å®ä¾‹åŒ–åˆ†è¯å™¨äº†ï¼Œå› ä¸ºæˆ‘ä»¬å¿…é¡»è¦ç¡®ä¿æ¥ä¸‹æ¥æˆ‘ä»¬åœ¨è‡ªå·±çš„ä»»åŠ¡ä¸­æ‰€ä½¿ç”¨çš„åˆ†è¯å™¨è¦å’Œè½½å…¥çš„é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨çš„åˆ†è¯å™¨æ˜¯åŒä¸€ä¸ªã€‚

å…¶æ¬¡ï¼Œåˆ†è¯å™¨æ¥ç€ä¼šå°†åˆ†å‰²åçš„è¯ï¼ˆè¯‘è€…æ³¨ï¼šå¯¹äºä¸­æ–‡æ¥è¯´å°±æ˜¯å­—ï¼‰è½¬æ¢æˆè¯è¡¨ä¸­çš„ç´¢å¼•ï¼Œè¿™æ ·åšçš„ç›®çš„å°±æ˜¯å°†æ–‡æœ¬è½¬æ¢æˆå‘é‡ï¼Œç„¶åå†å–‚ç»™æ¨¡å‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®çš„ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸€ä»½é¢å¤–çš„è¯è¡¨ï¼Œå¥½åœ¨å½“æˆ‘ä»¬é€šè¿‡æ–¹æ³•`from_pretrained()`å®ä¾‹åŒ–æ—¶ï¼Œè¿™ä¸ªè¯è¡¨å°±åœ¨åå°ä¸‹è½½äº†ã€‚

å¦‚æœéœ€è¦å¯¹ä¸€æ®µç»™å®šçš„æ–‡æœ¬è¿›è¡Œtokenizeï¼Œé‚£ä¹ˆåªéœ€è¦è¿è¡Œå¦‚ä¸‹ä»£ç å³å¯ï¼š

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
inputs = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
print(inputs)

#
{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

å¦‚ä¸Šæ‰€ç¤ºï¼Œè¿è¡Œåçš„ç»“æœå°†ä¼šè¿”å›ä¸€ä¸ªå­—å…¸ï¼Œå…¶åŒ…å«å¯¹åº”è¯åœ¨è¯è¡¨ä¸­çš„ç´¢å¼•ï¼ˆ [ids of the tokens](https://huggingface.co/transformers/master/glossary.html#input-ids)ï¼‰ï¼›ä»¥åŠæ¨¡å‹éœ€è¦ç”¨åˆ°çš„æ³¨æ„åŠ›æ©ç ï¼ˆ [attention mask](https://huggingface.co/transformers/master/glossary.html#attention-mask)ï¼‰ã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œå¦‚æœä½ æœ‰å¤šå¥æ ·æœ¬ï¼Œä½ è¿˜å¯ä»¥å°†å®ƒä»¬ä½œä¸ºä¸€ä¸ªbatché€šè¿‡ä¸€ä¸ª`lsit`æ¥ä¼ å…¥ã€‚æ­¤æ—¶ï¼Œä½ åº”è¯¥æŒ‡å®šéœ€è¦å°†è¿™ä¸€æ‰¹çš„æ ·æœ¬ä»¥å¤šå¤§çš„é•¿åº¦è¿›è¡Œæˆªå–ï¼ˆè¯‘è€…æ³¨ï¼š**å¦‚è¿‡æ²¡æœ‰æŒ‡å®šåˆ™é»˜è®¤ä»¥æœ€å¤§çš„é•¿åº¦**ï¼‰ï¼Œå¯¹äºå°äºæœ€å¤§é•¿åº¦çš„æ ·æœ¬åˆ™ä¼šè¿›è¡Œå¡«å……ï¼ˆè¯‘è€…æ³¨ï¼šä»¥0è¿›è¡Œå¡«å……ï¼‰ã€‚

```python
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    return_tensors="pt",
    #max_length=5   è‡ªå·±æŒ‡å®šæœ€å¤§é•¿åº¦
)

#å¦‚æœæ˜¯åœ¨TensorFlowç¯å¢ƒä¸­ï¼Œåªéœ€è¦å°† return_tensorsè®¾ç½®ä¸º "tf"å³å¯
```

æ•´ä¸ªå¡«å……çš„è¿‡ç¨‹å®Œå…¨æ˜¯æ ¹æ®æ¨¡å‹çš„éœ€è¦ï¼Œè‡ªåŠ¨é€‰æ‹©åœ¨å“ªè¾¹è¿›è¡Œå¡«å……çš„ï¼ˆåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ä¸ºå³è¾¹ï¼‰ï¼Œä¸éœ€è¦æˆ‘ä»¬è‡ªå·±å»è®¾å®šã€‚åœ¨è¿è¡Œå®Œä¸Šé¢çš„ä»£ç åï¼Œå°±èƒ½å¾—åˆ°å¦‚ä¸‹çš„ç»“æœï¼š

```python
for key, value in pt_batch.items():
    print(f"{key}: {value.numpy().tolist()}")
    
#
input_ids: [[101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 1012, 102], [101, 2057, 3246, 2017, 2123, 1005, 1056, 5223, 2009, 1012, 102, 0, 0, 0]]
attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]]
```

ä½ è¿˜å¯ä»¥é€šè¿‡ç‚¹å‡»è¿™ä¸ªé¡µé¢æ¥è·å–æ›´å¤šå…³äº[tokenizers](https://huggingface.co/transformers/master/preprocessing.html)çš„ä¿¡æ¯ã€‚

### 2.2 ä½¿ç”¨æ¨¡å‹

ä¸€æ—¦ä½ é€šè¿‡tokenizerå®Œæˆäº†å¯¹æ•°æ®çš„é¢„å¤„ç†å·¥ä½œï¼Œé‚£ä¹ˆä½ å°±å¯ä»¥ç›´æ¥å°†å…¶è¾“å…¥åˆ°å¯¹åº”çš„æ¨¡å‹ä¸­äº†ã€‚æ­£å¦‚æˆ‘ä»¬ä¸Šé¢ç¤ºä¾‹ä¸­æ‰€æåˆ°çš„ï¼Œåœ¨å®Œæˆtokenizeè¿™ä¸€æ­¥åæˆ‘ä»¬å°±ä¼šå¾—åˆ°æ‰€æœ‰æ¨¡å‹éœ€è¦è¾“å…¥çš„ä¸œè¥¿ã€‚å¦‚æœä½ ä½¿ç”¨çš„æ˜¯TensorFlowï¼Œé‚£ä¹ˆåªéœ€è¦å°†` tokenizer()`è¿”å›åçš„ç»“æœå–‚å…¥æ¨¡å‹å³å¯ï¼Œå¦‚æœæ˜¯Pytorchåˆ™éœ€è¦ç”¨`**`æ¥è¿›è¡Œè§£åŒ…ã€‚

```python
#é’ˆå¯¹Pytorch
pt_outputs = pt_model(**pt_batch)

#é’ˆå¯¹TensorFlow
tf_outputs = tf_model(tf_batch)
```

åœ¨*Transformers*ä¸­ï¼Œæ¨¡å‹çš„è¾“å‡ºç»“æœéƒ½æ˜¯`tuples`ç±»å‹çš„ï¼Œè¿™é‡Œæˆ‘ä»¬è¾“å‡ºæ¨¡å‹æœ€åä¸€å±‚çš„ç»“æœï¼š

```python
print(pt_outputs)
#
(tensor([[-4.0833,  4.3364],
        [ 0.0818, -0.0418]], grad_fn=<AddmmBackward>),)
```

å¯ä»¥çœ‹åˆ°ï¼Œæ¨¡å‹æœ€åè¿”å›ä»…ä»…åªæ˜¯è¿”å›äº†`tuples`ä¸­çš„ä¸€ä¸ªå…ƒç´ ï¼ˆæœ€åä¸€å±‚ï¼‰ï¼Œä½†æˆ‘ä»¬åŒæ ·å¯ä»¥è¿”å›å¤šå±‚çš„è¾“å‡ºç»“æœï¼Œè€Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆæ¨¡å‹è¿”å›çš„æ˜¯ä¸€ä¸ª`tuples`çš„åŸå› ã€‚

**æ³¨æ„ï¼š**æ‰€æœ‰çš„*Transformers*æ¨¡å‹ï¼ˆPytorchå’ŒTensorFlowï¼‰è¿”å›çš„æœ€åä¸€å±‚æŒ‡çš„éƒ½æ˜¯åœ¨æœ€åä¸€ä¸ªæ¿€æ´»å‡½æ•°å‰çš„å€¼ï¼ˆä¾‹å¦‚SoftMaxï¼‰ï¼Œå› ä¸ºé€šå¸¸æ¥è¯´æ­£çœŸæ„ä¹‰ä¸Šçš„æœ€åä¸€å±‚éƒ½æ˜¯å’ŒæŸå¤±å‡½æ•°ç»“åˆåœ¨ä¸€èµ·çš„ã€‚

è¯‘è€…æ³¨ï¼šå¦‚æœæ˜¯åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæœ€åä¸€å±‚æŒ‡çš„å°±æ˜¯logitsï¼Œå³æœ€åä¸€ä¸ªå…¨è¿æ¥å±‚çš„çº¿æ€§è¾“å‡ºç»“æœï¼ˆæ²¡æœ‰ç»è¿‡æ¿€æ´»å‡½æ•°ï¼‰ã€‚

ä¸‹é¢ï¼Œè®©æˆ‘ä»¬å°†ä¸Šé¢çš„è¾“å‡ºç»“æœè¾“å…¥åˆ°SoftMaxæ¿€æ´»å‡½æ•°ä¸­æ¥å¾—åˆ°ä¸€ä¸ªé¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒï¼š

```python
#é’ˆå¯¹Pytorch
import torch.nn.functional as F
pt_predictions = F.softmax(pt_outputs[0], dim=-1)

#é’ˆå¯¹TensorFlow
import tensorflow as tf
tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)
```

è¿™æ ·ï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿå¾—åˆ°é¢„æµ‹çš„ç»“æœï¼š

```python
#é’ˆå¯¹Pytorch
tensor([[2.2043e-04, 9.9978e-01],
        [5.3086e-01, 4.6914e-01]], grad_fn=<SoftmaxBackward>)

#é’ˆå¯¹TensorFlow
tf.Tensor(
[[2.2042994e-04 9.9977952e-01]
 [5.3086340e-01 4.6913657e-01]], shape=(2, 2), dtype=float32)
```

åŒæ—¶ï¼Œå¦‚æœä½ è¿˜æœ‰æ•°æ®æ ·æœ¬å¯¹åº”çš„æ ‡ç­¾ï¼Œé‚£ä¹ˆä½ è¿˜å¯ä»¥å°†å®ƒè¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œæ¨¡å‹å°±ä¼šè¿”å›ä¸€ä¸ªåŒ…å«æŸå¤±å€¼å’Œæœ€åä¸€å±‚çš„è¾“å‡ºå€¼ï¼š

```python
#é’ˆå¯¹ Pytorch
import torch
pt_outputs = pt_model(**pt_batch, labels = torch.tensor([1, 0]))
print(pt_outputs)
#
(tensor(0.3167, grad_fn=<NllLossBackward>), tensor([[-4.0833,  4.3364],
        [ 0.0818, -0.0418]], grad_fn=<AddmmBackward>))
#é’ˆå¯¹ TensorFlow
import tensorflow as tf
tf_outputs = tf_model(tf_batch, labels = tf.constant([1, 0]))
```

åœ¨*Transformers*ä¸­ï¼Œæ‰€æœ‰æ¨¡å‹çš„å®ç°éƒ½æ˜¯åŸºäº[`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)æˆ–è€…[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) çš„ï¼Œå› æ­¤ä½ ä¸€æ ·å¯ä»¥å°†å®ƒä»¬æ”¾åˆ°ä½ å¹³æ—¶è®­ç»ƒçš„ä»£ç ä¸­ã€‚åŒæ—¶ï¼Œ*Transformers*è¿˜æä¾›äº† [`Trainer`](https://huggingface.co/transformers/master/main_classes/trainer.html#transformers.Trainer) ï¼ˆ[`TFTrainer`](https://huggingface.co/transformers/master/main_classes/trainer.html#transformers.TFTrainer) å¦‚æœä½ ä½¿ç”¨çš„æ˜¯TensorFlowï¼‰ç±»æ¥å¸®åŠ©æˆ‘ä»¬è®­ç»ƒè‡ªå·±çš„æ•°æ®ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦ç­‰ã€‚å…³äºæ›´å¤šè®­ç»ƒç›¸å…³çš„ä»‹ç»ï¼Œè¯·ç‚¹å‡»è¿›å…¥é¡µé¢ [training tutorial](https://huggingface.co/transformers/master/training.html)ã€‚

**æ³¨æ„ï¼š**Pytorchæ¨¡å‹è¾“å‡ºçš„æ˜¯ç‰¹æ®Šçš„æ•°æ®ç±»å‹ï¼Œå› æ­¤ä½ å¯ä»¥åˆ©ç”¨IDEæ¥è‡ªåŠ¨è¡¥å…¨å¯¹åº”å±æ€§ã€‚

è¯‘è€…æ³¨ï¼šåœ¨Pytorchä¸­ï¼Œæ¨¡å‹è¾“å‡ºçš„ç»“æœéƒ½æ˜¯Tensorå¼ é‡ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€šè¿‡`.item()`å±æ€§æ¥å°†å…¶è¾“å‡ºç»“æœè½¬åŒ–ä¸ºæ•°å€¼ç±»å‹ã€‚ä¾‹å¦‚`print(pt_outputs[0].item())`ã€‚

### 2.3 ä¿å­˜æ¨¡å‹

ä¸€æ—¦ä½ å®Œæˆæ¨¡å‹çš„å¾®è°ƒåï¼Œä½ å¯ä»¥é€šè¿‡ä¸‹é¢çš„æ–¹æ³•æ¥ä¿å­˜å¯¹åº”çš„tokenizerå’Œæ¨¡å‹ï¼š

```python
tokenizer.save_pretrained(save_directory)
model.save_pretrained(save_directory)
```

è¯‘è€…æ³¨ï¼š`save_directory`ä¸ºæŒ‡å®šä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œæ²¡æœ‰å°†ä¼šè‡ªåŠ¨åˆ›å»ºã€‚

åœ¨è¿™ä¹‹åï¼Œä½ å¯ä»¥å‘å‰é¢ä»‹ç»çš„é‚£æ ·ï¼Œé€šè¿‡ [`from_pretrained()`](https://huggingface.co/transformers/master/model_doc/auto.html#transformers.AutoModel.from_pretrained)æ–¹æ³•æ¥è½½å…¥è®­ç»ƒå¥½çš„è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¤ç”¨ã€‚è€Œæ­¤æ—¶ä½ æ‰€éœ€è¦ä¼ å…¥`from_pretrained()`çš„å°±ä¸æ˜¯æ¨¡å‹çš„åå­—äº†ï¼Œè€Œæ˜¯ä½ ä¸Šé¢ä¿å­˜æ¨¡å‹å¯¹åº”çš„è·¯å¾„ã€‚åŒæ—¶ï¼Œåœ¨*Transformers*ä¸­ä¸€ä¸ªé…·ç‚«çš„åŠŸèƒ½å°±æ˜¯ï¼Œä¸ç®¡ä½ ä¸Šé¢ä¿å­˜å¥½çš„é¢„è®­ç»ƒæ¨¡å‹æ˜¯ç”¨`Pytorch`è¿˜æ˜¯`TensorFlow`è®­ç»ƒçš„ï¼Œä½ éƒ½èƒ½å°†å…¶è½½å…¥è¿›æ¥ç”¨äºä½ æ¥ä¸‹æ¥çš„å·¥ä½œä¸­ã€‚

å¦‚æœä½ è¦è½½å…¥ä¸€ä¸ªç”±`Pytorch`ä¿å­˜çš„æ¨¡å‹åˆ°`TensorFlow`çš„ç¯å¢ƒä¸­ï¼Œé‚£ä¹ˆåªéœ€è¦åƒä¸‹é¢è¿™æ ·ä½¿ç”¨å³å¯ï¼š

```python
tokenizer = AutoTokenizer.from_pretrained(save_directory)
model = TFAutoModel.from_pretrained(save_directory, from_pt=True)
```

å¦‚è¿‡ä½ è¦è½½å…¥ä¸€ä¸ªç”±`TensorFlow`ä¿å­˜çš„æ¨¡å‹åˆ°`Pytorch`çš„ç¯å¢ƒä¸­ï¼Œé‚£ä¹ˆåªéœ€è¦åƒä¸‹é¢è¿™æ ·ä½¿ç”¨å³å¯ï¼š

```python
tokenizer = AutoTokenizer.from_pretrained(save_directory)
model = AutoModel.from_pretrained(save_directory, from_tf=True)
```

æœ€åï¼Œä½ è¿˜å¯ä»¥è®©æ¨¡å‹è¿”å›æ‰€æœ‰éšè—çŠ¶æ€å’Œæ³¨æ„åŠ›æƒé‡ï¼Œå¦‚æœä½ éœ€è¦çš„è¯ï¼š

```python
#é’ˆå¯¹ Pytorch
pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)
all_hidden_states, all_attentions = pt_outputs[-2:]

#é’ˆå¯¹ TensorFlow
tf_outputs = tf_model(tf_batch, output_hidden_states=True, output_attentions=True)
all_hidden_states, all_attentions = tf_outputs[-2:]

```

### 2.4 è§¦æ‘¸ä»£ç 

åœ¨*Transformers*ä¸­ï¼Œ `AutoModel` å’Œ`AutoTokenizer`è¿™ä¸¤ä¸ªç±»ä»…ä»…åªæ˜¯ç”¨äºè‡ªé€‚åº”åŒ¹é…æ‰€è½½å…¥çš„æ¨¡å‹èƒŒåæ‰€å¯¹åº”çš„ç½‘ç»œæ¶æ„ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸ç®¡ä½ çš„é¢„è®­ç»ƒæ¨¡å‹æ˜¯é€šè¿‡*Transformers*ä¸­çš„å“ªç§ç½‘ç»œæ¶æ„è®­ç»ƒå¾—åˆ°çš„ï¼Œä½ éƒ½å¯ä»¥é€šè¿‡è¿™ä¸¤ä¸ªç±»æ¥è½½å…¥ä¸è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè€Œä¸ç”¨æ˜¾ç¤ºçš„æŒ‡å®šã€‚å› ä¸ºåœ¨è¿™èƒŒåï¼Œ*Transformers*ä¸ºæ¯ä¸€ç§ç½‘ç»œæ¨¡å‹ï¼ˆç»“æ„ï¼‰éƒ½å®šä¹‰å¥½äº†ä¸€ä¸ªæ¨¡å‹ç±»ï¼Œå› æ­¤ä½ å¯ä»¥è½»æ¾çš„è®¿é—®å’Œè°ƒæ•´ä»£ç ã€‚

ä¾‹å¦‚åœ¨æˆ‘ä»¬å‰é¢çš„ç¤ºä¾‹è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ‰€ä½¿ç”¨çš„æ¨¡å‹çš„åå­—æ˜¯`distilbert-base-uncased-finetuned-sst-2-english`ï¼Œè¿™æ„å‘³ç€è¯¥æ¨¡å‹æ˜¯é€šè¿‡[DistilBERT](https://huggingface.co/transformers/master/model_doc/distilbert.html) è¿™ä¸€ç½‘ç»œæ¶æ„è®­ç»ƒå¾—åˆ°çš„ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬åŒæ ·åªéœ€è¦é€šè¿‡ç±»[`AutoModelForSequenceClassification`](https://huggingface.co/transformers/master/model_doc/auto.html#transformers.AutoModelForSequenceClassification)ï¼ˆæˆ–è€… [`TFAutoModelForSequenceClassification`](https://huggingface.co/transformers/master/model_doc/auto.html#transformers.TFAutoModelForSequenceClassification) å¦‚æœä½ ä½¿ç”¨çš„æ˜¯TensorFlowï¼‰å°±èƒ½å¤Ÿå°†è¿™ä¸ªæ¨¡å‹ç»™è½½å…¥è¿›æ¥ï¼Œå¹¶ä¸”è‡ªåŠ¨åŒ¹é…åˆ°[`DistilBertForSequenceClassification`](https://huggingface.co/transformers/master/model_doc/distilbert.html#transformers.DistilBertForSequenceClassification)è¿™ä¸ªç±»ã€‚å…³äºè¿™å…¶ä¸­çš„ç»†èŠ‚ä¹‹å¤„ï¼Œä½ å¯ä»¥ç‚¹å‡»å‰é¢è¿™ä¸ªç±»å¯¹åº”çš„è¶…é“¾æ¥æˆ–è€…æ˜¯æŸ¥çœ‹ç›¸åº”çš„æºç æ¥è¿›è¡Œäº†è§£ã€‚

å½“ç„¶ï¼Œä½ åŒæ ·å¯ä»¥è‡ªå·±æ˜¾ç¤ºçš„æ¥è¿›è¡ŒæŒ‡å®šæ¨¡å‹æ‰€å¯¹åº”çš„ç±»ï¼Œä¾‹å¦‚åƒä¸‹é¢è¿™æ ·ä½¿ç”¨ï¼š

```python
# é’ˆå¯¹ Pytorch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = DistilBertForSequenceClassification.from_pretrained(model_name)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)

#é’ˆå¯¹ TensorFlow
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFDistilBertForSequenceClassification.from_pretrained(model_name)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
```

### 2.5 å®šåˆ¶ä½ è‡ªå·±çš„æ¨¡å‹

å¦‚æœä½ æƒ³æ›´æ”¹æ¨¡å‹ä¸­çš„ç»“æ„ï¼Œä½ è¿˜å¯ä»¥å®šä¹‰è‡ªå·±ç±»æ¥å¯¹ç½‘ç»œä¸­çš„å‚æ•°è¿›è¡Œé…ç½®ã€‚å¯¹äºæ¯ä¸€ä¸ªç½‘ç»œç»“æ„æ¥è¯´ï¼Œå®ƒéƒ½å¯¹åº”ç€ä¸€ä¸ªç›¸åº”é…ç½®ç±»ï¼ˆä¾‹å¦‚ DistilBERT,å®ƒçš„é…ç½®å¯¹åº”çš„å°±æ˜¯[`DistilBertConfig`](https://huggingface.co/transformers/master/model_doc/distilbert.html#transformers.DistilBertConfig)è¿™ä¸ªç±»ï¼‰ï¼Œè¿™ä¸ªç±»å…è®¸æˆ‘ä»¬æ¥æŒ‡å®šç½‘ç»œç»“æ„ä¸­çš„ä»»æ„ä¸€ä¸ªå‚æ•°ï¼Œä¾‹å¦‚éšè—å±‚ç»´åº¦ï¼ˆhidden dimensionï¼‰ã€ä¸¢å¼ƒç‡ç­‰ã€‚å¦‚æœä½ è¿˜æƒ³åšä¸€äº›æ ¸å¿ƒéƒ¨åˆ†çš„ä¿®æ”¹ï¼Œä¾‹å¦‚éšè—å±‚ä¸ªæ•°ï¼ˆhidden sizeï¼‰ï¼Œé‚£ä¹ˆä½ å°†éœ€è¦ä»å¤´å¼€å§‹åˆ©ç”¨ä½ è‡ªå·±æœé›†çš„æ•°æ®é›†æ¥è®­ç»ƒè¿™ä¸ªæ¨¡å‹ï¼Œè¿™å°±æ„å‘³ç€ä½ ä¸èƒ½å¤Ÿä½¿ç”¨ç°æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å®Œæˆè¿™äº›æ ¸å¿ƒéƒ¨åˆ†çš„ä¿®æ”¹åï¼Œæ¨¡å‹å°±ä¼šæŒ‰ç…§ä½ ä¿®æ”¹çš„é…ç½®è¢«é‡æ–°è¿›è¡Œå®ä¾‹åŒ–ç„¶åè®­ç»ƒã€‚

ä¸‹é¢ç¤ºä¾‹æ˜¯ä¸€ä¸ªä»å¤´å¼€å§‹è®­ç»ƒçš„DistilBERTç½‘ç»œæ¨¡å‹ï¼Œå…¶ä¸­Tokenizeræ‰€ç”¨åˆ°çš„è¯å…¸å¹¶æ²¡æœ‰å‘ç”Ÿå˜åŒ–ï¼Œå› æ­¤æˆ‘ä»¬è¿˜æ˜¯å¯ä»¥é€šè¿‡`from_pretrained()`æ–¹æ³•è¿›è¡Œè½½å…¥ã€‚

```python
# é’ˆå¯¹Pytorch
from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification(config)

# é’ˆå¯¹TensorFlow
from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification
config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4*512)
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = TFDistilBertForSequenceClassification(config)
```

å¯¹äºé‚£äº›ç»†å¾®çš„æ”¹åŠ¨è‡ªå¤„ï¼ˆä¾‹å¦‚æœ€åçš„åˆ†ç±»æ•°ï¼‰ï¼Œä½ åŒæ ·è¿˜æ˜¯å¯ä»¥ä½¿ç”¨è¿™äº›é¢„è®­ç»ƒæ¨¡å‹ã€‚ä¸‹é¢ï¼Œè®©æˆ‘ä»¬è½½å…¥ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹æ¥å®Œæˆä¸€ä¸ªååˆ†ç±»çš„ä»»åŠ¡ã€‚æ­¤æ—¶æˆ‘ä»¬éœ€è¦ä¿®æ”¹ä¸€äº›é…ç½®ï¼Œä¹Ÿå°±æ˜¯é™¤äº†åˆ†ç±»æ•°ä¹‹å¤–ï¼Œå…¶å®ƒçš„éƒ½ä¿æŒé»˜è®¤ä¸å˜ã€‚å¯¹äºè¿™æ­¥ï¼Œä½ å¯ä»¥ç®€å•çš„é€šè¿‡`from_pretrained()`æ–¹æ³•ä¼ é€’è¿›å»å¯¹åº”çš„å‚æ•°å³å¯ï¼š

```python
# é’ˆå¯¹ Pytorch
from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification
model_name = "distilbert-base-uncased"
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)

#é’ˆå¯¹ TensorFlow
from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification
model_name = "distilbert-base-uncased"
model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
```



## [<ä¸»é¡µ>](README.md)  

